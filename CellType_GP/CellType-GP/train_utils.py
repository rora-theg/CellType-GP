#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
train_utils.py
----------------
ç”¨äº CellType-GP æ¨¡å‹çš„é€šç”¨è®­ç»ƒå·¥å…·æ¨¡å—ã€‚
åŒ…å«ï¼š
    1. train_model_2() â€”â€” ä¸»è®­ç»ƒå‡½æ•°
    2. è‡ªåŠ¨ early stopping
    3. å­¦ä¹ ç‡åŠ¨æ€è°ƒæ•´ (ReduceLROnPlateau)
    4. è®­ç»ƒæ›²çº¿å¯è§†åŒ–ä¸æ—¥å¿—ä¿å­˜

ç‰ˆæœ¬: v1.0
"""

import os
import numpy as np
import torch
import matplotlib.pyplot as plt
from tqdm import trange


def train_model_2(
    model,
    Y_obs,
    num_epochs=3000,
    lr=1e-3,
    lambda1=1e-4,
    lambda2=1e-2,
    early_stop=True,
    patience=400,
    tol=1e-5,
    verbose=True,
    save_dir="./training_logs",
):
    """
    =============================================
    ä¸»è®­ç»ƒå‡½æ•°ï¼ˆå¸¦æ—©åœä¸å­¦ä¹ ç‡è°ƒåº¦ï¼‰
    =============================================

    å‚æ•°ï¼š
    ----------
    model : torch.nn.Module
        å·²å®šä¹‰å¥½çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œå¿…é¡»å®ç° model.loss(Y_obs, lambda1, lambda2) æ¥å£ï¼Œ
        è¿”å› (total_loss, recon_loss, l1_loss, smooth_loss)ã€‚

    Y_obs : torch.Tensor
        è§‚æµ‹æ•°æ®å¼ é‡ï¼Œå½¢çŠ¶é€šå¸¸ä¸º (programs Ã— spots)ã€‚

    num_epochs : int, é»˜è®¤ 3000
        è®­ç»ƒè½®æ¬¡ï¼ˆæœ€å¤§è¿­ä»£æ¬¡æ•°ï¼‰ã€‚

    lr : float, é»˜è®¤ 1e-3
        åˆå§‹å­¦ä¹ ç‡ã€‚

    lambda1 : float, é»˜è®¤ 1e-4
        ç¨€ç–çº¦æŸï¼ˆL1ï¼‰æ­£åˆ™é¡¹æƒé‡ã€‚

    lambda2 : float, é»˜è®¤ 1e-2
        ç©ºé—´å¹³æ»‘ï¼ˆLaplacianï¼‰æ­£åˆ™é¡¹æƒé‡ã€‚

    early_stop : bool, é»˜è®¤ True
        æ˜¯å¦å¯ç”¨æ—©åœæœºåˆ¶ï¼ˆloss é•¿æœŸæ— æ”¹å–„è‡ªåŠ¨åœæ­¢ï¼‰ã€‚

    patience : int, é»˜è®¤ 400
        æ—©åœå®¹å¿æ¬¡æ•°ï¼Œå³è¿ç»­å¤šå°‘ä¸ª epoch æ²¡æœ‰æ”¹å–„ååœæ­¢ã€‚

    tol : float, é»˜è®¤ 1e-5
        æ”¹å–„é˜ˆå€¼ï¼Œå°äºè¯¥å€¼çš„æ³¢åŠ¨è§†ä¸ºâ€œæ— æå‡â€ã€‚

    verbose : bool, é»˜è®¤ True
        æ˜¯å¦æ‰“å°è®­ç»ƒè¿›åº¦æ¡ä¸æŸå¤±ä¿¡æ¯ã€‚

    save_dir : str, é»˜è®¤ "./training_logs"
        ä¿å­˜æ—¥å¿—ä¸æ›²çº¿å›¾çš„ç›®å½•ã€‚

    è¿”å›ï¼š
    ----------
    history : dict
        åŒ…å«è®­ç»ƒå†å²çš„å­—å…¸ï¼š
            - 'total_loss': æ€»æŸå¤±ï¼ˆå«æ­£åˆ™ï¼‰
            - 'recon_loss': é‡æ„è¯¯å·®
            - 'l1_loss': L1 æ­£åˆ™é¡¹
            - 'smooth_loss': ç©ºé—´å¹³æ»‘æ­£åˆ™é¡¹
            - 'lr': å­¦ä¹ ç‡å˜åŒ–
            - 'best_loss': æœ€ä¼˜ loss å€¼
            - 'best_epoch': å¯¹åº” epoch
    """

    # ========== åˆå§‹åŒ– ==========
    os.makedirs(save_dir, exist_ok=True)  # ç¡®ä¿ä¿å­˜ç›®å½•å­˜åœ¨

    optimizer = torch.optim.Adam(model.parameters(), lr=lr)  
    # æ¯æ¬¡åå‘ä¼ æ’­åï¼Œç”¨ Adam ç®—æ³•ã€å­¦ä¹ ç‡ = lrï¼Œæ¥è°ƒæ•´ model çš„å‚æ•°

    # å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼šå½“éªŒè¯é›†çš„ loss é•¿æœŸä¸æ”¹å–„æ—¶ï¼Œè‡ªåŠ¨å°†å­¦ä¹ ç‡å‡åŠ
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode="min", factor=0.5, patience=max(1, patience // 4) ,min_lr=1e-5
    )

    # è®°å½•è®­ç»ƒè¿‡ç¨‹
    history = {"total_loss": [], "recon_loss": [],  "l1_loss":[], "smooth_loss":[] ,"lr": [] }

    best_loss = np.inf  # æœ€ä¼˜lossåˆå§‹åŒ–ä¸ºæ— ç©·å¤§
    best_epoch = 0      # è®°å½•æœ€ä¼˜epoch
    no_improve = 0      # æ— æå‡è®¡æ•°å™¨

    # tqdm è¿›åº¦æ¡
    pbar = trange(num_epochs, disable=not verbose, ncols=100)

    # ========== ä¸»è®­ç»ƒå¾ªç¯ ==========
    for epoch in pbar:
        # 1. å‰å‘ä¼ æ’­ + è®¡ç®—loss
        total_loss, recon_loss, l1_loss, smooth_loss = model.loss(Y_obs, lambda1, lambda2)


        # 2. åå‘ä¼ æ’­
        optimizer.zero_grad()
        total_loss.backward()
        optimizer.step()

        # 3. è®°å½•å½“å‰å­¦ä¹ ç‡ä¸loss
        current_lr = optimizer.param_groups[0]["lr"]
        history["total_loss"].append(total_loss.item())
        history["recon_loss"].append(recon_loss.item())
        history["l1_loss"].append(l1_loss.item() if hasattr(l1_loss, 'item') else float(l1_loss))
        history["smooth_loss"].append(smooth_loss.item() if hasattr(smooth_loss, 'item') else float(smooth_loss))
        history["lr"].append(current_lr)

        # 4. æ›´æ–°å­¦ä¹ ç‡è°ƒåº¦å™¨
        scheduler.step(total_loss.item())

        # 5. è¿›åº¦æ¡æ˜¾ç¤ºå¹¶æ‰“å°ä¸­é—´å±‚
        if verbose:
            pbar.set_description(
                f"Epoch {epoch:4d} | Loss {total_loss.item():.4f} | Recon {recon_loss.item():.4f} | LR {current_lr:.2e}"
            )

        if epoch % 500 == 0:  # æ¯500è½®æ‰“å°ä¸€æ¬¡ä¸­é—´å±‚ç»Ÿè®¡
            with torch.no_grad():
                Y_pred = model.forward()
                print(f"ğŸ” [ä¸­é—´å±‚æ£€æŸ¥] Epoch {epoch}")
                print(f"  Y_tps  mean={model.Y_tps.mean().item():.6e}, std={model.Y_tps.std().item():.6e}")
                print(f"  Y_pred mean={Y_pred.mean().item():.6e}, std={Y_pred.std().item():.6e}")


        # 6. æ—©åœé€»è¾‘
        if early_stop:
            if total_loss.item() < best_loss - tol:
                best_loss = total_loss.item()
                best_epoch = epoch
                no_improve = 0
                # ä¿å­˜å½“å‰æ¨¡å‹å‚æ•°
                torch.save(model.state_dict(), os.path.join(save_dir, "best_model.pt"))
            else:
                no_improve += 1

            if no_improve > patience:
                if verbose:
                    print(f"\nğŸ”¹ æ—©åœè§¦å‘ï¼šåœ¨ç¬¬ {epoch} è½®åœæ­¢è®­ç»ƒï¼Œæœ€ä¼˜loss={best_loss:.6f}")
                break

    # ========== è®­ç»ƒç»“æŸåå¤„ç† ==========
    history = {k: np.array(v) for k, v in history.items()}
    history["best_loss"] = best_loss
    history["best_epoch"] = best_epoch

    # ä¿å­˜lossæ›²çº¿
    fig_path = os.path.join(save_dir, "loss_curve.png")
    plt.figure(figsize=(7, 5))
    plt.plot(history["total_loss"], label="Total Loss", color="tab:blue")
    plt.plot(history["recon_loss"], label="Reconstruction Loss", color="tab:orange")
    plt.axvline(best_epoch, color="red", linestyle="--", label=f"Best Epoch {best_epoch}")
    plt.xlabel("Epoch")
    plt.ylabel("Loss")
    plt.legend()
    plt.title("Total Loss Curve")
    plt.tight_layout()
    plt.savefig(fig_path, dpi=300)
    plt.close()

    # ä¿å­˜æ—¥å¿—æ–‡ä»¶
    log_path = os.path.join(save_dir, "train_log.txt")
    with open(log_path, "w", encoding="utf-8") as f:
        f.write("====== è®­ç»ƒå‚æ•°ä¸ç»“æœæ—¥å¿— ======\n")
        f.write(f"å­¦ä¹ ç‡åˆå§‹å€¼: {lr}\n")
        f.write(f"Î»1 (ç¨€ç–æ­£åˆ™): {lambda1}\n")
        f.write(f"Î»2 (ç©ºé—´å¹³æ»‘æ­£åˆ™): {lambda2}\n")
        f.write(f"æœ€å¤§è½®æ¬¡: {num_epochs}\n")
        f.write(f"æ—©åœé˜ˆå€¼: {tol}, å®¹å¿æ¬¡æ•°: {patience}\n")
        f.write(f"æœ€ä¼˜Epoch: {best_epoch}\n")
        f.write(f"æœ€ä¼˜Loss: {best_loss:.6f}\n")
        f.write(f"æœ€ç»ˆå­¦ä¹ ç‡: {history['lr'][-1]:.6e}\n")
        f.write("\n====== å†å²Losså‰5æ¡è®°å½• ======\n")
        for i in range(min(5, len(history['total_loss']))):
            f.write(f"Epoch {i}: loss={history['total_loss'][i]:.6f}\n")
        f.write("\nè®­ç»ƒå®Œæˆï¼\n")

    if verbose:
        print(f"\nâœ… è®­ç»ƒå®Œæˆï¼šæœ€ä¼˜loss={best_loss:.6f}ï¼Œæœ€ä¼˜epoch={best_epoch}")
        print(f"ğŸ“‰ Lossæ›²çº¿å·²ä¿å­˜è‡³: {fig_path}")
        print(f"ğŸ“ è®­ç»ƒæ—¥å¿—å·²ä¿å­˜è‡³: {log_path}")

    return history


# ==========
