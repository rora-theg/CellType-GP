#!/usr/bin/env python

"""
å•æ–¹æ³•è¯„ä¼°è„šæœ¬ï¼šè¯„ä¼°é¢„æµ‹çš„ CTGP æ•°å€¼ä¸çœŸå®å€¼çš„æ‹Ÿåˆåº¦ä¸å­˜åœ¨æ€§åˆ¤æ–­ã€‚
"""

import argparse
from pathlib import Path
from typing import Dict, Optional, Tuple

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

try:
    # sklearn å¯ç”¨äºé¢å¤–æŒ‡æ ‡ï¼ˆå¦‚ ROC-AUCï¼‰ï¼Œè‹¥ä¸å¯ç”¨åˆ™é™çº§
    from sklearn.metrics import roc_auc_score, average_precision_score

    _has_sklearn = True
except Exception:
    _has_sklearn = False


def load_tables(truth_path: Path, pred_path: Path) -> Tuple[pd.DataFrame, pd.DataFrame]:
    """è¯»å–çœŸå€¼ä¸é¢„æµ‹è¡¨ï¼ŒæŒ‰ç…§ç´¢å¼•å’Œåˆ—è¿›è¡Œå¯¹é½ã€‚"""
    # `index_col=0` è¯»å–æ—¶æŠŠç¬¬ä¸€åˆ—ä½œä¸º spot ç´¢å¼•ï¼Œä¿æŒä¸å®½è¡¨æ ¼å¼ä¸€è‡´ã€‚
    truth = pd.read_csv(truth_path, index_col=0)
    pred = pd.read_csv(pred_path, index_col=0)

    # å¯¹é½è¡Œåˆ—ï¼Œä¿è¯ä¸€ä¸€å¯¹åº”
    truth, pred = truth.align(pred, join="inner", axis=0)
    truth, pred = truth.align(pred, join="inner", axis=1)
    return truth, pred


def compute_regression_metrics(truth: pd.DataFrame, pred: pd.DataFrame) -> Tuple[Dict[str, float], pd.DataFrame]:
    """è®¡ç®—æ‹Ÿåˆåº¦æŒ‡æ ‡ï¼ŒåŒ…æ‹¬æ•´ä½“ä¸æŒ‰ç‰¹å¾åˆ’åˆ†çš„ç»“æœã€‚"""
    # stack() å°†å®½è¡¨è½¬æ¢ä¸ºé•¿è¡¨ï¼šç´¢å¼•å˜ä¸º (spot, feature)ï¼Œä¾¿äºæ‰¹é‡è®¡ç®—ã€‚
    truth_stack = truth.stack().rename("truth")
    pred_stack = pred.stack().rename("pred")

    # åˆå¹¶ä¸¤åˆ—å¹¶ä¸¢å¼ƒ NaNï¼Œåªè¯„ä¼°åŒæ–¹åŒæ—¶æœ‰å€¼çš„ spot-feature å¯¹ã€‚
    merged = pd.concat([truth_stack, pred_stack], axis=1, join="inner").dropna()

    metrics: Dict[str, float] = {}
    if len(merged) > 1:
        metrics["pearson"] = merged["truth"].corr(merged["pred"], method="pearson")  # çº¿æ€§ç›¸å…³ç³»æ•°
        metrics["spearman"] = merged["truth"].corr(merged["pred"], method="spearman")  # ç§©ç›¸å…³
    else:
        metrics["pearson"] = np.nan
        metrics["spearman"] = np.nan

    diff = merged["pred"] - merged["truth"]
    metrics["mae"] = float(np.abs(diff).mean()) if len(diff) else np.nan  # å¹³å‡ç»å¯¹è¯¯å·®
    metrics["rmse"] = float(np.sqrt(np.mean(diff**2))) if len(diff) else np.nan  # å‡æ–¹æ ¹è¯¯å·®
    metrics["n_pairs"] = len(merged)  # æ ·æœ¬æ•°ï¼Œç”¨äºåˆ¤æ–­æ˜¯å¦æœ‰è¶³å¤Ÿæ¯”è¾ƒç‚¹

    # æŒ‰åˆ—ï¼ˆCTGPï¼‰ç»Ÿè®¡ï¼Œä¾¿äºæ·±å…¥åˆ†æ
    per_feature = []
    for feature, group in merged.groupby(level=1):
        # group åŒ…å«åŒä¸€ CTGP åœ¨æ‰€æœ‰ spot çš„æ¯”è¾ƒç»“æœ
        if len(group) > 1:
            pearson = group["truth"].corr(group["pred"], method="pearson")
            spearman = group["truth"].corr(group["pred"], method="spearman")
        else:
            pearson = np.nan
            spearman = np.nan
        diff_f = group["pred"] - group["truth"]
        per_feature.append(
            {
                "feature": feature,
                "pearson": pearson,
                "spearman": spearman,
                "mae": float(np.abs(diff_f).mean()),
                "rmse": float(np.sqrt(np.mean(diff_f**2))),
                "n_pairs": len(group),
            }
        )

    per_feature_df = pd.DataFrame(per_feature).sort_values("pearson", ascending=False)
    return metrics, per_feature_df


def compute_presence_metrics(truth: pd.DataFrame, pred: pd.DataFrame, threshold: float) -> Tuple[Dict[str, float], pd.DataFrame]:
    """å¯¹æ¯”å„æ–¹æ³•åœ¨é¢„æµ‹â€œæ˜¯å¦å­˜åœ¨â€ä¸Šçš„è¡¨ç°ã€‚"""
    # çœŸå€¼ä¸­ NaN ä»£è¡¨â€œä¸å­˜åœ¨â€ï¼Œå› æ­¤ notna() èƒ½ç›´æ¥æ„å»ºå­˜åœ¨æ ‡ç­¾ã€‚
    truth_presence = truth.notna().stack().astype(int).rename("truth_presence")
    # é¢„æµ‹å€¼å…ˆå¡«å……ç¼ºå¤±ä¸º 0ï¼Œå† stack() å¾—åˆ°ä¸çœŸå€¼åŒç»“æ„çš„é•¿è¡¨ã€‚
    pred_presence = pred.fillna(0).stack().rename("pred_score")

    # å¯¹é½çœŸå€¼æ ‡ç­¾ä¸é¢„æµ‹åˆ†æ•°ï¼Œå¹¶åŸºäºé˜ˆå€¼è½¬æ¢ä¸ºäºŒåˆ†ç±»ç»“æœã€‚
    aligned = pd.concat([truth_presence, pred_presence], axis=1, join="inner")
    aligned["pred_presence"] = (aligned["pred_score"] > threshold).astype(int)

    # ç»Ÿè®¡æ··æ·†çŸ©é˜µæ¡ç›®ï¼Œç”¨äºè®¡ç®—å¸¸è§äºŒåˆ†ç±»æŒ‡æ ‡ã€‚
    tp = int(((aligned["pred_presence"] == 1) & (aligned["truth_presence"] == 1)).sum())
    fp = int(((aligned["pred_presence"] == 1) & (aligned["truth_presence"] == 0)).sum())
    tn = int(((aligned["pred_presence"] == 0) & (aligned["truth_presence"] == 0)).sum())
    fn = int(((aligned["pred_presence"] == 0) & (aligned["truth_presence"] == 1)).sum())

    precision = tp / (tp + fp) if (tp + fp) else 0.0
    recall = tp / (tp + fn) if (tp + fn) else 0.0
    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) else 0.0
    accuracy = (tp + tn) / (tp + tn + fp + fn) if (tp + tn + fp + fn) else 0.0

    prob_metrics = {"roc_auc": np.nan, "average_precision": np.nan}
    if _has_sklearn and aligned["truth_presence"].nunique() > 1:
        try:
            prob_metrics["roc_auc"] = roc_auc_score(aligned["truth_presence"], aligned["pred_score"])
            prob_metrics["average_precision"] = average_precision_score(aligned["truth_presence"], aligned["pred_score"])
        except Exception:
            pass

    summary = {
        "tp": tp,
        "fp": fp,
        "tn": tn,
        "fn": fn,
        "precision": precision,
        "recall": recall,
        "f1": f1,
        "accuracy": accuracy,
        **prob_metrics,
    }

    # æŒ‰ç‰¹å¾ç»Ÿè®¡ï¼Œå¸®åŠ©å®šä½é—®é¢˜ CTGP
    per_feature_rows = []
    for feature, group in aligned.groupby(level=1):
        tp_f = int(((group["pred_presence"] == 1) & (group["truth_presence"] == 1)).sum())
        fp_f = int(((group["pred_presence"] == 1) & (group["truth_presence"] == 0)).sum())
        tn_f = int(((group["pred_presence"] == 0) & (group["truth_presence"] == 0)).sum())
        fn_f = int(((group["pred_presence"] == 0) & (group["truth_presence"] == 1)).sum())

        prec_f = tp_f / (tp_f + fp_f) if (tp_f + fp_f) else 0.0
        rec_f = tp_f / (tp_f + fn_f) if (tp_f + fn_f) else 0.0
        f1_f = 2 * prec_f * rec_f / (prec_f + rec_f) if (prec_f + rec_f) else 0.0
        acc_f = (tp_f + tn_f) / (tp_f + tn_f + fp_f + fn_f) if (tp_f + tn_f + fp_f + fn_f) else 0.0

        per_feature_rows.append(
            {
                "feature": feature,
                "tp": tp_f,
                "fp": fp_f,
                "tn": tn_f,
                "fn": fn_f,
                "precision": prec_f,
                "recall": rec_f,
                "f1": f1_f,
                "accuracy": acc_f,
            }
        )

    per_feature_df = pd.DataFrame(per_feature_rows).sort_values("f1", ascending=False)
    return summary, per_feature_df





def main() -> None:
    parser = argparse.ArgumentParser(description="è¯„ä¼°å•ä¸ª CTGP é¢„æµ‹ç»“æœã€‚")
    parser.add_argument("--truth", type=Path, required=True, help="çœŸå®å®½è¡¨ CSV è·¯å¾„")
    parser.add_argument("--prediction", type=Path, required=True, help="é¢„æµ‹å®½è¡¨ CSV è·¯å¾„")
    parser.add_argument("--method-name", type=str, default="method", help="æ–¹æ³•åç§°ï¼Œç”¨äºè¾“å‡ºæ ‡è¯†")
    parser.add_argument("--presence-threshold", type=float, default=0.0, help="åˆ¤æ–­å­˜åœ¨æ€§çš„é˜ˆå€¼")
    parser.add_argument("--scatter-path", type=Path, default=None, help="æ•£ç‚¹å›¾è¾“å‡ºè·¯å¾„ï¼ˆç¼ºçœä¸ºä»…å±•ç¤ºï¼‰")
    parser.add_argument("--summary-path", type=Path, default=None, help="æ•´ä½“æŒ‡æ ‡è¾“å‡º CSV è·¯å¾„")
    parser.add_argument("--per-feature-regression", type=Path, default=None, help="æŒ‰ç‰¹å¾è¾“å‡ºæ‹ŸåˆæŒ‡æ ‡ CSV è·¯å¾„")
    parser.add_argument("--per-feature-presence", type=Path, default=None, help="æŒ‰ç‰¹å¾è¾“å‡ºå­˜åœ¨æ€§æŒ‡æ ‡ CSV è·¯å¾„")
    parser.add_argument("--no-show", action="store_true", help="åªä¿å­˜å›¾åƒï¼Œä¸å¼¹å‡ºçª—å£")

    args = parser.parse_args()

    # Step1: ç»Ÿä¸€å¯¹é½çœŸå€¼ä¸é¢„æµ‹è¡¨ï¼Œé˜²æ­¢é”™è¡Œé”™åˆ—ã€‚
    truth, pred = load_tables(args.truth, args.prediction)

    # 1) æ‹Ÿåˆåº¦è¯„ä¼°
    # å°†æ•´ä½“æŒ‡æ ‡æ‰“å°åˆ°æ§åˆ¶å°ï¼Œä¾¿äºå¿«é€Ÿè§‚å¯Ÿã€‚
    reg_summary, reg_per_feature = compute_regression_metrics(truth, pred)
    print("===== æ‹Ÿåˆåº¦æŒ‡æ ‡ =====")
    for key, value in reg_summary.items():
        print(f"{key:>10}: {value}")

    # å¦‚éœ€æŒ‰ç‰¹å¾ä¿å­˜æŒ‡æ ‡ï¼Œå†™å…¥ CSVï¼Œæ–¹ä¾¿åç»­åœ¨ notebook ä¸­è¿‡æ»¤åˆ†æã€‚
    if args.per_feature_regression:
        args.per_feature_regression.parent.mkdir(parents=True, exist_ok=True)
        reg_per_feature.to_csv(args.per_feature_regression, index=False)
        print(f"ğŸ“„ å·²ä¿å­˜æŒ‰ç‰¹å¾æ‹ŸåˆæŒ‡æ ‡ï¼š{args.per_feature_regression}")

    # æ•£ç‚¹å›¾ä½¿ç”¨ä¸Šä¸€å‡½æ•°ç”Ÿæˆéœ€è¦çš„æ•°æ®
    merged_data = pd.concat(
        [
            truth.stack().rename("truth"),
            pred.stack().rename("pred"),
        ],
        axis=1,
        join="inner",
    ).dropna()
    # show å‚æ•°å¯åœ¨ notebook ä¸­ä¿æŒäº¤äº’ï¼Œä¹Ÿå¯ç”¨äºæ‰¹å¤„ç†ä¿å­˜å›¾ç‰‡ã€‚
    plot_regression_scatter(merged_data, args.method_name, args.scatter_path, show=not args.no_show)

    # 2) å­˜åœ¨æ€§è¯„ä¼°
    # presence_threshold æ§åˆ¶â€œé¢„æµ‹å€¼å¤šå¤§è§†ä½œå­˜åœ¨â€ï¼Œæ»¡è¶³å®éªŒéœ€æ±‚ã€‚
    presence_summary, presence_per_feature = compute_presence_metrics(truth, pred, args.presence_threshold)
    print("\n===== å­˜åœ¨æ€§æŒ‡æ ‡ =====")
    for key, value in presence_summary.items():
        print(f"{key:>10}: {value}")

    if args.per_feature_presence:
        args.per_feature_presence.parent.mkdir(parents=True, exist_ok=True)
        presence_per_feature.to_csv(args.per_feature_presence, index=False)
        print(f"ğŸ“„ å·²ä¿å­˜æŒ‰ç‰¹å¾å­˜åœ¨æ€§æŒ‡æ ‡ï¼š{args.per_feature_presence}")

    # ç»¼åˆç»“æœè¾“å‡º
    if args.summary_path:
        args.summary_path.parent.mkdir(parents=True, exist_ok=True)
        summary_df = pd.DataFrame(
            [
                {
                    "method": args.method_name,
                    **{f"reg_{k}": v for k, v in reg_summary.items()},
                    **{f"presence_{k}": v for k, v in presence_summary.items()},
                    "threshold": args.presence_threshold,
                }
            ]
        )
        summary_df.to_csv(args.summary_path, index=False)
        # æ±‡æ€»æ–‡ä»¶ä¾¿äºè·¨æ–¹æ³•æ¯”è¾ƒæ—¶ç›´æ¥æ‹¼æ¥æˆ–åˆå¹¶å¤šè¡Œè®°å½•ã€‚
        print(f"ğŸ“„ å·²ä¿å­˜æ•´ä½“æŒ‡æ ‡æ±‡æ€»ï¼š{args.summary_path}")


if __name__ == "__main__":
    main()
