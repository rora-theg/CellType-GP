#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
å¹¶è¡Œ/å¤šå¡ç‰ˆï¼šåŸºäº train_model_2 çš„ç½‘æ ¼è°ƒå‚ + å¯¼å‡º ctGP å®½è¡¨
----------------------------------------------------------------
- å¯¹æ¯ä¸ª (Î»1, Î»2, lr) ç»„åˆï¼š
  1) é‡æ–°åˆ›å»ºæ¨¡å‹
  2) è°ƒç”¨ train_model_2 è®­ç»ƒï¼ˆæ—©åœ+è°ƒåº¦ï¼‰
  3) åŠ è½½ best_model.pt
  4) ä» model.Y_tps å¯¼å‡º ctGP å®½è¡¨ (S x T*P)ï¼Œå¹¶ç”¨å‚æ•°å‘½åä¿å­˜
- å¤šè¿›ç¨‹å¹¶è¡Œï¼šè‡ªåŠ¨æŒ‰ GPU æ•°é‡è½®è¯¢åˆ†é… CUDA_VISIBLE_DEVICES
- åªä¾èµ–ä½ çš„ train_utils.py

ç”¨æ³•ç¤ºä¾‹ï¼š
  python grid_search_parallel_export.py \
      --npz ./DATA/spot_data_full.npz \
      --out ./grid_train_utils_parallel \
      --gpus 0,1 \
      --epochs 3000 --patience 200 \
      --l1 1e-5 5e-5 1e-4 5e-4 \
      --l2 5e-3 1e-2 2e-2 5e-2 \
      --lr 1e-3
"""

import os
import time
import json
from pathlib import Path
from itertools import product
import multiprocessing as mp

import numpy as np
import pandas as pd
import torch

from train_utils import train_model_2


# ================ éœ€è¦ä½ æŒ‰é¡¹ç›®å®ç°çš„éƒ¨åˆ† ================
def create_model():
    """
    è¿”å›â€œå…¨æ–°åˆå§‹åŒ–â€çš„æ¨¡å‹å®ä¾‹ï¼ˆæ¯ä¸ªç»„åˆå¿…é¡»æ˜¯æ–°å®ä¾‹ï¼‰
    TODO: å°†æ­¤å¤„æ›¿æ¢ä¸ºä½ çš„æ¨¡å‹æ„é€ ï¼Œä¾‹å¦‚ï¼š
        from my_model import CTGPModel
        return CTGPModel(T=..., P=..., S=..., ...)
    """
    raise NotImplementedError("è¯·å®ç° create_model() è¿”å›ä¸€ä¸ªæ–°çš„æ¨¡å‹å®ä¾‹ã€‚")


# ================ å·¥å…·å‡½æ•° ================
def fmt(x: float) -> str:
    """æŠŠæµ®ç‚¹æ•°å˜æˆæ–‡ä»¶åå‹å¥½çš„ç§‘å­¦è®¡æ•°æ³•ï¼ˆä¾‹å¦‚ 1e-03ï¼‰"""
    return f"{x:.2e}".replace("+", "").replace("0e", "e")


def make_tag(lr, l1, l2, ep):
    """ç»„åˆæ ‡ç­¾"""
    return f"ep{ep}_lr{fmt(lr)}_l1{fmt(l1)}_l2{fmt(l2)}"


def load_npz(npz_path: Path):
    """
    è¯»å– npzï¼Œè¿”å›ï¼š
      Y_obs (torch.Tensor)  â€”â€” è®­ç»ƒè§‚æµ‹ï¼›æ³¨æ„ç»´åº¦éœ€å’Œä½ çš„ model.loss åŒ¹é…
      spot_names            â€”â€” (S,)
      celltype_names        â€”â€” (T,)
      program_names         â€”â€” (P,)
    ä½ çš„ npz åº”åŒ…å«ï¼š
      - visium_score: (P, S)
      - spot_names, celltype_names, program_names
    """
    data = np.load(npz_path, allow_pickle=True)
    # è§†ä½ çš„æ¨¡å‹çº¦å®šå†³å®šæ˜¯å¦éœ€è¦è½¬ç½®
    Y_obs_np = data["visium_score"]  # (P, S)
    Y_obs = torch.tensor(Y_obs_np, dtype=torch.float32)

    spot_names = data["spot_names"]
    celltype_names = data["celltype_names"]
    program_names = data["program_names"]
    return Y_obs, spot_names, celltype_names, program_names


def export_ctgp_wide(model, spot_names, celltype_names, program_names, out_csv: Path):
    """
    ä»æ¨¡å‹å¯¼å‡º Y_tps -> å®½è¡¨å¹¶ä¿å­˜
    çº¦å®šï¼šmodel åœ¨è®­ç»ƒåæš´éœ² model.Y_tps (T, P, S)
    å¦‚æœä½ çš„æ¨¡å‹æ²¡æœ‰å¸¸é©»å±æ€§ï¼Œè¯·æ”¹ä¸º model.compute_Y_tps(...)
    """
    if not hasattr(model, "Y_tps"):
        raise RuntimeError("æ¨¡å‹ç¼ºå°‘ Y_tps å±æ€§ï¼Œè¯·åœ¨æ¨¡å‹ä¸­æä¾› Y_tps æˆ–æš´éœ² compute_Y_tps() æ–¹æ³•ã€‚")

    Y_tps = model.Y_tps.detach().cpu().numpy()  # (T, P, S)
    T, P, S = Y_tps.shape

    # å½¢çŠ¶æ£€æŸ¥
    assert len(celltype_names) == T, f"Tä¸åŒ¹é…: {T} vs {len(celltype_names)}"
    assert len(program_names) == P, f"Pä¸åŒ¹é…: {P} vs {len(program_names)}"
    assert len(spot_names) == S, f"Sä¸åŒ¹é…: {S} vs {len(spot_names)}"

    # è½¬å®½è¡¨ (S, T*P)
    Y_tps_flat = np.transpose(Y_tps, (2, 0, 1)).reshape(S, T * P)
    columns = [f"{ct}+{pg}" for ct in celltype_names for pg in program_names]
    df = pd.DataFrame(Y_tps_flat, index=spot_names, columns=columns)
    out_csv.parent.mkdir(parents=True, exist_ok=True)
    df.to_csv(out_csv, index=True)
    return df.shape


# ================ å•ä¸ªç»„åˆçš„å·¥ä½œè¿›ç¨‹ ================
def run_one_combo(args):
    """
    å­è¿›ç¨‹æ‰§è¡Œå‡½æ•°ï¼š
      - è®¾å®š GPU ç¯å¢ƒ
      - åˆ›å»ºæ¨¡å‹å¹¶è®­ç»ƒ
      - åŠ è½½ best_model å¹¶å¯¼å‡º ctGP å®½è¡¨
      - å†™ meta.json
    """
    (npz_path, base_outdir, dataset_tag, method_tag,
     lr, l1, l2, num_epochs, patience, tol, seed,
     gpu_id) = args

    # è¿›ç¨‹å†…è®¾ç½®éšæœºç§å­
    torch.manual_seed(seed)
    np.random.seed(seed)

    # è¿›ç¨‹å†…è®¾ç½® GPUï¼ˆå¦‚æœæœ‰ï¼‰
    if gpu_id is not None:
        os.environ["CUDA_VISIBLE_DEVICES"] = str(gpu_id)
        device = torch.device("cuda:0")
    else:
        device = torch.device("cpu")

    # è½½å…¥æ•°æ®
    Y_obs, spot_names, celltype_names, program_names = load_npz(Path(npz_path))
    Y_obs = Y_obs.to(device)

    # ç”Ÿæˆç›®å½•ä¸æ ‡ç­¾
    tag = make_tag(lr, l1, l2, num_epochs)
    run_dir = Path(base_outdir) / f"{dataset_tag}_{method_tag}_{tag}"
    run_dir.mkdir(parents=True, exist_ok=True)

    # åˆ›å»ºæ¨¡å‹
    model = create_model().to(device)

    t0 = time.time()
    try:
        history = train_model_2(
            model,
            Y_obs=Y_obs,
            num_epochs=num_epochs,
            lr=lr,
            lambda1=l1,
            lambda2=l2,
            early_stop=True,
            patience=patience,
            tol=tol,
            verbose=False,
            save_dir=str(run_dir),
        )
    except Exception as e:
        return {
            "status": "failed",
            "error": str(e),
            "lambda1": l1, "lambda2": l2, "lr": lr,
            "num_epochs": num_epochs, "patience": patience, "tol": tol,
            "run_dir": str(run_dir)
        }

    # åŠ è½½æœ€ä¼˜æƒé‡
    best_pt = run_dir / "best_model.pt"
    if best_pt.exists():
        model.load_state_dict(torch.load(best_pt, map_location=device))
    model.eval()

    # å¯¼å‡º ctGP å®½è¡¨
    ctgp_csv = run_dir / f"ctgp_{tag}.csv"
    shape = export_ctgp_wide(model, spot_names, celltype_names, program_names, ctgp_csv)

    # å†™ meta
    meta = {
        "dataset": dataset_tag,
        "method": method_tag,
        "lambda1": l1, "lambda2": l2, "lr": lr,
        "num_epochs": num_epochs, "patience": patience, "tol": tol,
        "best_loss": float(history["best_loss"]),
        "best_epoch": int(history["best_epoch"]),
        "final_lr": float(history["lr"][-1]) if len(history["lr"]) else float(lr),
        "seconds": round(time.time() - t0, 2),
        "ctgp_csv": str(ctgp_csv),
        "ctgp_shape": list(shape),
        "run_dir": str(run_dir),
        "status": "ok",
    }
    with open(run_dir / "meta.json", "w", encoding="utf-8") as f:
        json.dump(meta, f, ensure_ascii=False, indent=2)

    return meta


# ================ å¹¶è¡Œè°ƒåº¦å…¥å£ ================
def main():
    import argparse
    ap = argparse.ArgumentParser(description="å¹¶è¡Œ/å¤šå¡ç½‘æ ¼è°ƒå‚ + å¯¼å‡º ctGP å®½è¡¨ï¼ˆåŸºäº train_model_2ï¼‰")
    ap.add_argument("--npz", type=Path, required=True, help="åŒ…å« visium_score/spot_names/celltype_names/program_names çš„ npz è·¯å¾„")
    ap.add_argument("--out", type=Path, default=Path("./grid_train_utils_parallel"), help="è¾“å‡ºæ ¹ç›®å½•")
    ap.add_argument("--dataset", type=str, default="visium", help="æ•°æ®æ ‡ç­¾ï¼ˆä»…ç”¨äºå‘½åï¼‰")
    ap.add_argument("--method", type=str, default="train_utils_only", help="æ–¹æ³•æ ‡ç­¾ï¼ˆä»…ç”¨äºå‘½åï¼‰")

    ap.add_argument("--epochs", type=int, default=3000)
    ap.add_argument("--patience", type=int, default=200)
    ap.add_argument("--tol", type=float, default=1e-5)
    ap.add_argument("--seed", type=int, default=42)

    ap.add_argument("--l1", type=float, nargs="+", default=[1e-5, 5e-5, 1e-4, 5e-4], help="lambda1 å€™é€‰åˆ—è¡¨")
    ap.add_argument("--l2", type=float, nargs="+", default=[5e-3, 1e-2, 2e-2, 5e-2], help="lambda2 å€™é€‰åˆ—è¡¨")
    ap.add_argument("--lr", type=float, nargs="+", default=[1e-3], help="å­¦ä¹ ç‡å€™é€‰åˆ—è¡¨")

    ap.add_argument("--gpus", type=str, default="", help="é€—å·åˆ†éš”çš„ GPU id åˆ—è¡¨ï¼Œå¦‚ '0,1,2'ï¼›ç•™ç©ºè¡¨ç¤ºå…¨ç”¨CPU")
    ap.add_argument("--workers", type=int, default=0, help="å¹¶è¡Œè¿›ç¨‹æ•°ï¼›é»˜è®¤æ ¹æ® GPU æ•°è‡ªåŠ¨ç¡®å®š")
    args = ap.parse_args()

    # è®¾å¤‡åˆ—è¡¨ä¸å¹¶è¡Œåº¦
    gpu_list = [g.strip() for g in args.gpus.split(",") if g.strip() != ""]
    if len(gpu_list) == 0:
        # çº¯ CPU å¹¶è¡Œ
        devices = [None] * (args.workers if args.workers > 0 else max(1, mp.cpu_count() // 2))
    else:
        # å¤šå¡ï¼šé»˜è®¤æ¯å¼ å¡å¼€ä¸€ä¸ªè¿›ç¨‹ï¼›ä¹Ÿå¯ä»¥ç”¨ --workers æ§åˆ¶æ›´å¤§å¹¶å‘
        n = args.workers if args.workers > 0 else len(gpu_list)
        devices = [gpu_list[i % len(gpu_list)] for i in range(n)]

    # ç»„åˆåˆ—è¡¨
    combos = list(product(args.l1, args.l2, args.lr))
    args.out.mkdir(parents=True, exist_ok=True)

    # æ„é€ ä»»åŠ¡ï¼ˆæŠŠç»„åˆè½®è¯¢åˆ†é…ç»™ä¸åŒè®¾å¤‡ï¼‰
    tasks = []
    for idx, (l1, l2, lr) in enumerate(combos):
        gpu_id = devices[idx % len(devices)] if len(devices) > 0 else None
        tasks.append((
            str(args.npz),
            str(args.out),
            args.dataset,
            args.method,
            float(lr), float(l1), float(l2),
            int(args.epochs), int(args.patience), float(args.tol), int(args.seed),
            gpu_id
        ))

    print(f"ğŸš€ æ€»ç»„åˆæ•°: {len(combos)} | å¹¶è¡Œåº¦: {len(devices)} | è®¾å¤‡: {devices if devices else ['cpu']}")

    # å¹¶è¡Œè·‘
    mp.set_start_method("spawn", force=True)
    with mp.Pool(processes=len(devices)) as pool:
        results = list(pool.map(run_one_combo, tasks))

    # æ±‡æ€» CSV
    df = pd.DataFrame(results)
    df.to_csv(args.out / "grid_summary.csv", index=False)
    print(f"\nğŸ“„ æ±‡æ€»å·²ä¿å­˜: {args.out / 'grid_summary.csv'}")

    # æ‰“å°æœ€ä¼˜
    ok = df[df["status"] == "ok"]
    if len(ok):
        best = ok.sort_values("best_loss", ascending=True).iloc[0]
        print("\nğŸ† æœ€ä¼˜ç»„åˆï¼ˆæŒ‰ best_loss æœ€å°ï¼‰ï¼š")
        print(
            f"  Î»1={best['lambda1']}  Î»2={best['lambda2']}  lr={best['lr']}\n"
            f"  best_loss={best['best_loss']:.6f} @ epoch {int(best['best_epoch'])}\n"
            f"  å¯¼å‡ºæ–‡ä»¶: {best['ctgp_csv']}\n  ç›®å½•: {best['run_dir']}"
        )
    else:
        print("â—æ²¡æœ‰æˆåŠŸç»“æœï¼Œè¯·æ£€æŸ¥å„ run_dir ä¸‹çš„ train_log.txtã€‚")


if __name__ == "__main__":
    main()
