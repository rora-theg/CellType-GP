"""
celltype_gp_models.py
---------------------
æ¨¡å—åŠŸèƒ½ï¼š
    åŸºäºä¸åŒæ¨¡å‹æ–¹æ³•è®¡ç®—ç»†èƒç±»å‹ç‰¹å¼‚æ€§åŸºå› ç¨‹åºè´¡çŒ® (Y_tps)
    å¹¶è½¬æ¢ä¸ºä¸ truth_result å¯¹åº”çš„å®½æ ¼å¼ç»“æœè¡¨ã€‚

Inputs:
    npz æ–‡ä»¶éœ€åŒ…å«ï¼š
        - visium_score: (P, S) ç¨‹åºæ‰“åˆ†çŸ©é˜µï¼ˆæ¯åˆ—ä¸ºä¸€ä¸ª spotï¼‰
        - spot_cluster_fraction_matrix: (S, T) ç»†èƒç±»å‹æ¯”ä¾‹çŸ©é˜µ
        - coords: (S, 2) ç©ºé—´åæ ‡ï¼ˆæœ¬æ–‡ä»¶å½“å‰æœªä½¿ç”¨ï¼‰
        - spot_names, celltype_names, program_names

Method notes / æ–¹æ³•è¯´æ˜ï¼š
    - "vectorized" (é»˜è®¤)ï¼šåŠ æ€§è´¡çŒ®åˆ†è§£ï¼ˆAdditive contributionï¼‰ã€‚
        å…ˆç”¨ Ridge å›å½’æ‹Ÿåˆ Y â‰ˆ XÂ·Î²ï¼›éšåå°†é¢„æµ‹å€¼æŒ‰ç»†èƒç±»å‹åˆ†è§£ä¸º
        å¯¹æ¯ä¸ª t çš„è´¡çŒ®é¡¹ï¼šcontrib_t(s,p) = X_s,t Â· Î²_t,pã€‚
        è¯¥åˆ†è§£ä¸çº¿æ€§æ¨¡å‹å¯åŠ æ€§ä¸€è‡´ã€ç¨³å®šã€å‘é‡åŒ–é«˜æ•ˆã€‚

    - "lofo_refit"ï¼šé€ç»†èƒç±»å‹ LOFO é‡æ‹Ÿåˆï¼ˆLeave-one-feature-outï¼‰ã€‚
        å¯¹æ¯ä¸ª tï¼ŒåŸºäº X_-t æ‹Ÿåˆ Î²_-t å¹¶è®¡ç®—æ®‹å·® Y - X_-tÂ·Î²_-tã€‚
        è¯¥æ–¹æ³•æ›´è´´è¿‘â€œå‰”é™¤ t åå‰©ä½™è§£é‡ŠåŠ›â€ï¼Œä½†è®¡ç®—å¼€é”€ä¸º O(T)ã€‚

"""

import numpy as np
import pandas as pd
import torch
from sklearn.linear_model import Ridge



# ====================================================
# ğŸ§  æ ¸å¿ƒå‡½æ•°ï¼šä¸åŒè®¡ç®—æ–¹æ³•
# ====================================================

def _standardize_columns(X: np.ndarray) -> tuple[np.ndarray, np.ndarray, np.ndarray]:
    """
    åˆ—æ ‡å‡†åŒ–ï¼ˆzero-mean, unit-varianceï¼‰ï¼Œè¿”å› (X_std, mean, std)ã€‚
    Column-wise standardization for stable Ridge fitting.
    """
    mu = X.mean(axis=0, keepdims=True)
    sigma = X.std(axis=0, keepdims=True) + 1e-6
    return (X - mu) / sigma, mu, sigma

def clr_transform(X: np.ndarray, eps: float = 1e-6) -> np.ndarray:
    """å¯¹ç»„åˆæ•°æ®çŸ©é˜µåš CLR å˜æ¢ã€‚"""
    X_safe = np.clip(X, eps, None)
    geom_mean = np.exp(np.mean(np.log(X_safe), axis=1, keepdims=True))
    return np.log(X_safe / geom_mean)

def contribution_vectorized(Y: np.ndarray, X: np.ndarray, alpha: float = 0.1, 
                            use_clr: bool = False,eps: float = 1e-6,
                            standardize: bool = False,
                            positive: bool = True) -> np.ndarray:
    """
    å‘é‡åŒ–åŠ æ€§è´¡çŒ®åˆ†è§£ï¼ˆé»˜è®¤æ–¹æ³•ï¼‰
    Vectorized additive contribution decomposition.

    å‚æ•° / Args:
        Y: (P, S) ç¨‹åºå¾—åˆ†çŸ©é˜µï¼Œæ¯åˆ—ä¸€ä¸ª spotã€‚
        X: (S, T) ç»†èƒç±»å‹æ¯”ä¾‹çŸ©é˜µã€‚
        alpha: Ridge æ­£åˆ™ç³»æ•°ã€‚
        use_clr: æ˜¯å¦å¯¹ X åš CLR å˜æ¢ï¼ˆå»ºè®® Trueï¼‰ï¼Œæ¶ˆé™¤ç»†èƒæ¯”ä¾‹çŸ©é˜µçš„å®Œå…¨å…±çº¿æ€§é—®é¢˜ã€‚
        eps: CLR å˜æ¢ä¸­çš„æœ€å°å€¼æˆªæ–­ï¼Œé˜²æ­¢ log(0ï¼‰ã€‚
        standardize: æ˜¯å¦å¯¹ X åšåˆ—æ ‡å‡†åŒ–ï¼ˆå»ºè®® Trueï¼‰ã€‚
        positive: æ˜¯å¦å¯¹ Ridge å›å½’çš„ç³»æ•°æ–½åŠ éè´Ÿçº¦æŸã€‚

    è¿”å› / Returns:
        Y_tps: (T, P, S)ï¼Œæ¯ä¸ªç»†èƒç±»å‹ t å¯¹ (ç¨‹åº p, spot s) çš„åŠ æ€§è´¡çŒ®ã€‚

    è¯´æ˜ / Notes:
        å…ˆæ‹Ÿåˆ Y â‰ˆ XÂ·Î²ï¼ˆmulti-target Ridgeï¼‰ï¼ŒéšåæŒ‰ t å°†é¢„æµ‹åˆ†è§£ä¸ºè´¡çŒ®é¡¹ï¼š
        contrib[t, p, s] = X_std[s, t] * Î²[t, p]ï¼›æ‰€æœ‰ t çš„ contrib æ±‚å’Œå³ä¸º Y_hatã€‚
        è¯¥åˆ†è§£ä¸çº¿æ€§æ¨¡å‹å¯åŠ æ€§ç›¸ç¬¦ï¼Œæ¯” LOFO æ®‹å·®æ›´ç¨³å®šä¸”æ˜“å‘é‡åŒ–ã€‚

        å¦‚æœåç»­éœ€è¦å¯¹ç»“æœè¿›è¡Œè§£é‡Šï¼Œéœ€å°†CLRç©ºé—´çš„Î²ç³»æ•°åæ˜ å°„å›æ¯”ä¾‹ç©ºé—´ã€‚
    """
    P, S = Y.shape
    S2, T = X.shape
    assert S == S2, f"S mismatch: Y(P,S)={Y.shape} vs X(S,T)={X.shape}"

    X_in = X.copy()
    if use_clr:
        X_in = clr_transform(X_in,eps)
    if standardize:
        X_in, _, _ = _standardize_columns(X_in)


    # sklearn æœŸæœ›å½¢çŠ¶ï¼šfit(X: (S,T), Y: (S,P))
    ridge = Ridge(alpha=alpha, positive=positive)
    ridge.fit(X_in, Y.T)
    # sklearn.coef_ å½¢çŠ¶ä¸º (n_targets=P, n_features=T)
    beta = ridge.coef_.T  # (T, P)


    # è´¡çŒ®åˆ†è§£ï¼šå¯¹æ¯ä¸ª tï¼Œcontrib_t(s,p) = X_in[s,t] * beta[t,p]
    # é€šè¿‡çˆ±å› æ–¯å¦æ±‚å’Œå‘é‡åŒ–ï¼š
    # è¾“å…¥ (S,T) ä¸ (T,P)ï¼Œè¾“å‡º (T,P,S)
    contrib = np.einsum('st,tp->tps', X_in, beta, optimize=True)
    return contrib.astype(np.float32, copy=False)


def lofo_refit_residual(Y: np.ndarray, X: np.ndarray, alpha: float = 0.1, 
                        use_clr: bool = False,eps: float = 1e-6,
                        standardize: bool = False) -> np.ndarray:
    """
    é€ t LOFO é‡æ‹Ÿåˆå¹¶è®¡ç®—æ®‹å·®ï¼šY_tps[t,:,:] = Y - X_-tÂ·Î²_-tã€‚
    æ”¯æŒ CLR ä¸æ ‡å‡†åŒ–ã€‚
    True LOFO residual per feature (slower, O(T) fits)).
    """
    P, S = Y.shape
    S2, T = X.shape
    assert S == S2, f"S mismatch: Y(P,S)={Y.shape} vs X(S,T)={X.shape}"

    Y_tps = np.empty((T, P, S), dtype=np.float32)
    for t in range(T):
        mask = np.ones(T, dtype=bool)
        mask[t] = False
        X_other = X[:, mask]
        Xin = X_other


        if use_clr:
            Xin = clr_transform(Xin, eps)
        if standardize:
            Xin, _, _ = _standardize_columns(Xin)
        
        ridge = Ridge(alpha=alpha)
        ridge.fit(Xin, Y.T)                     # (S, T-1) -> (S, P)
        Y_hat_other = ridge.predict(Xin).T      # -> (P, S)
        Y_tps[t] = (Y - Y_hat_other).astype(np.float32)
    return Y_tps


# ç©ºé—´ä¾èµ–æ€§å»ºæ¨¡æ³•ï¼ˆå–æ¶ˆï¼‰

# ====================================================
# ğŸ§¾ å·¥å…·å‡½æ•°
# ====================================================

def Ytps_to_wide_df(Y_tps, spot_names, celltype_names, program_names):
    """
    æŠŠ Y_tps (T,P,S) è½¬ä¸º truth_result æ ·å¼çš„å®½è¡¨æ ¼ï¼ˆS, T*Pï¼‰ã€‚
    Tolerates numpy æˆ– torch è¾“å…¥ï¼Œè¾“å‡ºä¸º pandas.DataFrameã€‚
    """
    # å…è®¸ torch/numpy ä¸¤ç§è¾“å…¥
    if isinstance(Y_tps, torch.Tensor):
        T, P, S = Y_tps.shape
        Y_np = Y_tps.permute(2, 0, 1).detach().cpu().numpy().reshape(S, T * P)
    else:
        Y_np = np.asarray(Y_tps)
        T, P, S = Y_np.shape
        Y_np = np.transpose(Y_np, (2, 0, 1)).reshape(S, T * P)

    columns = [f"{ct}+{pg}" for ct in celltype_names for pg in program_names]
    return pd.DataFrame(Y_np, index=spot_names, columns=columns)


# ====================================================
# ğŸš€ ä¸»å‡½æ•°å…¥å£
# ====================================================

def run_model(npz_path, method: str = "vectorized", save_path: str = "train_result.csv",
              alpha: float = 0.1, standardize: bool = False , use_clr: bool = False,
              positive: bool = True) -> pd.DataFrame:
    """
    è¿è¡Œ ctGP åˆ†è§£ï¼Œå¹¶å¯¼å‡ºå®½è¡¨ã€‚

    å‚æ•° / Args:
        npz_path: è¾“å…¥ npz è·¯å¾„ï¼Œéœ€å« visium_score/X/åå­—ç­‰å­—æ®µã€‚
        method: 'vectorized'ï¼ˆé»˜è®¤ï¼Œå¯è§£é‡ŠåŠ æ€§åˆ†è§£ï¼‰æˆ– 'lofo_refit'ï¼ˆé€ t é‡æ‹Ÿåˆæ®‹å·®ï¼‰ã€‚
        save_path: è¾“å‡º CSV è·¯å¾„ã€‚
        alpha: Ridge æ­£åˆ™ç³»æ•°ã€‚
        standardize: æ˜¯å¦æ ‡å‡†åŒ– X çš„åˆ—ã€‚
        use_clrï¼šæ˜¯å¦å¯¹XåšCLRå˜æ¢ã€‚
    """
    data = np.load(npz_path, allow_pickle=True)
    Y = data["visium_score"].astype(np.float64)                 # (P, S)
    X = data["spot_cluster_fraction_matrix"].astype(np.float64)  # (S, T)
    # coords ç›®å‰æœªä½¿ç”¨ï¼›å¦‚æ‰©å±•ç©ºé—´é¡¹ï¼Œå¯åœ¨æ­¤ä¼ å…¥
    # coords = data.get("coords")
    spot_names = data["spot_names"]
    celltype_names = data["celltype_names"]
    program_names = data["program_names"]

    if method == "vectorized":
        Y_tps = contribution_vectorized(Y, X, alpha=alpha, standardize=standardize, use_clr=use_clr, eps=1e-6,positive=positive)
    elif method == "lofo_refit":
        Y_tps = lofo_refit_residual(Y, X, alpha=alpha, standardize=standardize, use_clr=use_clr, eps=1e-6,positive=positive)
    else:
        raise ValueError("method å¿…é¡»æ˜¯ ['vectorized', 'lofo_refit'] ä¹‹ä¸€")

    df = Ytps_to_wide_df(Y_tps, spot_names, celltype_names, program_names)
    df.to_csv(save_path)
    print(f"âœ… å·²ä¿å­˜ç»“æœåˆ° {save_path}ï¼Œå½¢çŠ¶ {df.shape}")
    return df



# ====================================================
# ğŸ§­ å‘½ä»¤è¡Œå…¥å£ï¼ˆå¯é€‰ï¼‰
# ====================================================
if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser(description="Run celltype-GP model.")
    parser.add_argument("--input", type=str, required=True, help="Path to npz file")
    parser.add_argument("--method", type=str, default="vectorized", choices=["vectorized", "lofo_refit"], help="Method name")
    parser.add_argument("--save", type=str, default="train_result.csv", help="Output path")
    parser.add_argument("--alpha", type=float, default=0.1, help="Ridge regularization strength")
    parser.add_argument("--no-standardize", action="store_true", help="Disable column standardization for X")
    args = parser.parse_args()
    run_model(args.input, args.method, args.save, alpha=args.alpha, standardize=not args.no_standardize)
