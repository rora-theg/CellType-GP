"""
è®¡ç®— Xenium æ•°æ®çš„çœŸå®åŸºå› ç¨‹åºå¾—åˆ†ï¼ˆtruth scoreï¼‰
å‡½æ•°åŒ–ç‰ˆæœ¬ï¼šå¯æ¸…æ´—ã€åˆ†ç»„ã€é€è§†ã€ä¿å­˜
ä½œè€…ï¼šthegï¼ˆæœ¬æ¬¡æ”¹åŠ¨ï¼šå»äº¤äº’åŒ–å…¥å£ã€ä½¿ç”¨æ¸…æ´—åçš„ DataFrameã€å¯é…ç½®åˆ†ç»„åˆ—åï¼‰
"""

import scanpy as sc
import pandas as pd
import os
from sklearn.preprocessing import MinMaxScaler


# ========== å‡½æ•°å®šä¹‰åŒº ==========

def clean_obs_data(adata, drop_columns: list[str] = None):
    """
    æ¸…æ´— adata.obsï¼š
    - è¾“å‡ºæ‰€æœ‰åˆ—åä¸ºä¸€è¡Œå­—ç¬¦ä¸²ï¼ˆå¯ç›´æ¥å¤åˆ¶ç²˜è´´ï¼‰
    - ç”¨æˆ·è¾“å…¥è¦åˆ é™¤çš„åˆ—ï¼ˆç›´æ¥ç²˜è´´å³å¯ï¼‰
    - è‡ªåŠ¨åˆ é™¤å¹¶è¾“å‡ºæ¸…æ´—ç»“æœ
    """

    df = adata.obs.copy()

    # 1ï¸âƒ£ è¾“å‡ºåˆ—åï¼Œæ–¹ä¾¿å¤åˆ¶ç²˜è´´
    col_list = ", ".join(df.columns)
    print("\nğŸ§¾ å½“å‰ adata.obs åˆ—å¦‚ä¸‹ï¼ˆå¯ç›´æ¥å¤åˆ¶ç²˜è´´ï¼‰ï¼š\n")
    print(col_list)
    print("\nğŸ’¡ æç¤ºï¼šä½ å¯ä»¥å¤åˆ¶ä¸Šé¢è¿™ä¸€è¡Œï¼Œç„¶åç²˜è´´è¦åˆ é™¤çš„åˆ—ï¼ˆæˆ–éƒ¨åˆ†åˆ—ï¼‰")

    # 2ï¸âƒ£ å»äº¤äº’åŒ–ï¼ˆå¦‚æœªæä¾› drop_columnsï¼Œåˆ™ä¸åˆ é™¤ï¼›ä¿ç•™äº¤äº’ä½œä¸º fallbackï¼‰
    if drop_columns is None:
        try:
            user_input = input("\nè¯·è¾“å…¥è¦åˆ é™¤çš„åˆ—ï¼ˆå¤šä¸ªç”¨é€—å·åˆ†éš”ï¼Œç›´æ¥å›è½¦åˆ™è·³è¿‡åˆ é™¤ï¼‰:\n> ").strip()
            if user_input:
                drop_columns = [c.strip() for c in user_input.split(",") if c.strip()]
            else:
                drop_columns = []
        except Exception:
            # éäº¤äº’ç¯å¢ƒï¼šå¿½ç•¥åˆ é™¤
            drop_columns = []

    # 3ï¸âƒ£ æ‰§è¡Œåˆ é™¤
    if drop_columns:
        df.drop(columns=drop_columns, inplace=True, errors='ignore')
        print(f"\nğŸ§¹ å·²åˆ é™¤ {len(drop_columns)} åˆ—ï¼š{', '.join(drop_columns)}")
    else:
        print("\nâœ… æœªåˆ é™¤ä»»ä½•åˆ—")

    # 4ï¸âƒ£ é¢å¤–æ¸…ç†
    if "Unnamed: 0" in df.columns:
        df.drop("Unnamed: 0", axis=1, inplace=True)

    print(f"âœ¨ æ¸…æ´—å®Œæˆï¼šä¿ç•™ {df.shape[1]} åˆ—ã€‚\n")
    return df


def compute_group_means(df: pd.DataFrame, spot_col: str, celltype_col: str,
                        suffix: str, ## ç›®æ ‡åˆ—åç¼€ï¼Œè‡ªå®šä¹‰
                        normalize_within_spot:bool = True
                        ):
    """
    æŒ‰ Visium barcode(spot_col) + broad_annotation(celltype_col) åˆ†ç»„ï¼Œè®¡ç®—å„ _score åˆ—çš„å¹³å‡å€¼
    è¿”å› group å¹³å‡ä¸è®¡æ•°ã€‚
    å‚æ•°ï¼š
        df : pd.DataFrame
            åŒ…å« spot å’Œ celltype çš„æ•°æ®æ¡†ã€‚
        spot_col : str
            æ–‘ç‚¹ï¼ˆbarcodeï¼‰åˆ—åã€‚
        celltype_col : str
            ç»†èƒç±»å‹åˆ—åã€‚
        suffix : str, é»˜è®¤ '_score'
            ç›®æ ‡åˆ—çš„åç¼€ï¼ˆå¦‚ '_score'ã€'_norm'ï¼‰ï¼Œç”¨äºç­›é€‰è®¡ç®—åˆ—ã€‚
    
    è¿”å›ï¼š
        pd.DataFrame : åˆå¹¶çš„å¹³å‡å€¼ + è®¡æ•° DataFrameã€‚
    """

    # ç­›é€‰ä»¥ suffix ç»“å°¾çš„åˆ—
    # ç­›é€‰ç›®æ ‡åˆ—
    score_cols = [col for col in df.columns if col.endswith(suffix)]
    if len(score_cols) == 0:
        raise ValueError(f"âŒ æœªæ‰¾åˆ°ä»¥ '{suffix}' ç»“å°¾çš„åˆ—ã€‚")

    print(f"ğŸ§© æ£€æµ‹åˆ° {len(score_cols)} ä¸ªç›®æ ‡åˆ—ï¼ˆåç¼€ '{suffix}'ï¼‰")

    grouped_means = (
        df.groupby([spot_col, celltype_col], observed=False)[score_cols]
        .mean()
        .reset_index()
    )
    grouped_counts = (
        df.groupby([spot_col, celltype_col], observed=False)
        .size()
        .reset_index(name='cell_count')
    )
    truth_result = pd.merge(grouped_means, grouped_counts, on=[spot_col, celltype_col])

    # âœ… Spot å†…å½’ä¸€åŒ–
    if normalize_within_spot:
        print("âš™ï¸ æ­£åœ¨å¯¹æ¯ä¸ª spot å†… gene program å‡å€¼è¿›è¡Œ MinMax å½’ä¸€åŒ– ...")
        def normalize_group(x):
            scaler = MinMaxScaler()
            x[score_cols] = scaler.fit_transform(x[score_cols])
            return x
        truth_result = truth_result.groupby(spot_col, group_keys=False, observed=False).apply(normalize_group)
        print("âœ… å½’ä¸€åŒ–å®Œæˆï¼ˆæŒ‰æ¯ä¸ª spot è¿›è¡Œ MinMax ç¼©æ”¾ï¼‰")

    print(f"âœ… åˆ†ç»„å¹³å‡å®Œæˆ: {truth_result.shape[0]} è¡Œ")
    return truth_result

                       
def pivot_truth_scores(truth_result: pd.DataFrame, spot_col: str, celltype_col: str,
                       suffix:str ):
    """å°† truth_result è½¬ä¸ºå®½è¡¨ (spot Ã— celltype+program)"""
    program_cols = [c for c in truth_result.columns if c.endswith(suffix)]

    truth_wide = truth_result.pivot_table(index=spot_col, columns=celltype_col, values=program_cols)

    # å±•å¼€å¤šçº§åˆ—å
    truth_wide.columns = [f"{ctype}+{pg}" for pg, ctype in truth_wide.columns]
    truth_wide = truth_wide.reset_index().rename(columns={spot_col: "spot"})

    print(f"âœ… å®½è¡¨å®Œæˆ: {truth_wide.shape[0]} Ã— {truth_wide.shape[1]}")
    return truth_wide


def save_truth_outputs(df_clean: pd.DataFrame,
                       truth_result: pd.DataFrame,
                       truth_wide: pd.DataFrame,
                       output_dir: str):
    """ä¿å­˜æ‰€æœ‰ç»“æœæ–‡ä»¶"""
    os.makedirs(output_dir, exist_ok=True)
    path_score = os.path.join(output_dir, "truth_score.csv")
    path_result = os.path.join(output_dir, "truth_result_grouped.csv")
    path_wide = os.path.join(output_dir, "truth_result(wide).csv")

    df_clean.to_csv(path_score, index=False)
    truth_result.to_csv(path_result, index=False)
    truth_wide.to_csv(path_wide, index=False)

    print("ğŸ’¾ ä¿å­˜ç»“æœï¼š")
    print(f"  â”œâ”€ ç»†èƒçº§ truth_scoreï¼š{path_score}")
    print(f"  â”œâ”€ åˆ†ç»„å‡å€¼ truth_resultï¼š{path_result}")
    print(f"  â””â”€ å®½è¡¨ truth_result(wide)ï¼š{path_wide}")


# ========== ä¸»å‡½æ•°å…¥å£ ==========

def compute_truth_score(adata, output_dir: str = "./truth_output",
                       spot_col: str = 'transcript_level_visium_barcode',
                       celltype_col: str = 'broad_annotation',
                       drop_columns: list[str] = None,
                       suffix: str = '_score'):
    """
    ä»å·²åŠ è½½çš„ Xenium AnnData å¯¹è±¡è®¡ç®—çœŸå®åŸºå› ç¨‹åºå¾—åˆ†ã€‚
    å‚æ•°:
        adata : å·²åŠ è½½çš„ AnnData å¯¹è±¡
        output_dir : è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤ ./truth_outputï¼‰
    """
    print(f"ğŸš€ å¼€å§‹è®¡ç®— Xenium truth score, è¾“å‡ºè·¯å¾„: {output_dir}")

    # Step 1ï¸âƒ£ æ¸…æ´— obsï¼ˆéäº¤äº’ç¯å¢ƒå¯ä¼ å…¥ drop_columns=None ä»¥ä¿æŒåŸæ ·ï¼‰
    df_clean = clean_obs_data(adata, drop_columns=drop_columns)

    # Step 2ï¸âƒ£ è®¡ç®—åˆ†ç»„å‡å€¼ï¼ˆä½¿ç”¨æ¸…æ´—åçš„ df_cleanï¼‰
    truth_result = compute_group_means(df_clean, spot_col=spot_col, celltype_col=celltype_col, suffix=suffix)

    # Step 3ï¸âƒ£ è½¬å®½è¡¨
    truth_wide = pivot_truth_scores(truth_result, spot_col=spot_col, celltype_col=celltype_col, suffix=suffix)

    # Step 4ï¸âƒ£ ä¿å­˜ç»“æœ
    save_truth_outputs(df_clean, truth_result, truth_wide, output_dir)

    print("\nğŸ‰ Xenium truth score è®¡ç®—å®Œæˆï¼")
    return truth_wide
